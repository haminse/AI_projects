{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cde91b",
   "metadata": {},
   "source": [
    "# 작사가 인공지능 만들기\n",
    "\n",
    "## 필요 모듈 임포트 및 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9721af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 파일을 읽기모드로 열고\n",
    "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9539e94",
   "metadata": {},
   "source": [
    "## 극 대본에서 인물 대사만 추려오기\n",
    "공백 라인과 배역 소개 라인을 지웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d9350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus): #raw_corpus는 한 라인씩 할당된 리스트\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53069d8e",
   "metadata": {},
   "source": [
    "## 정규표현식으로 코퍼스 다듬기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237eae5",
   "metadata": {},
   "source": [
    "입력된 문장을\n",
    "1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "2. 특수문자 양쪽에 공백을 넣고\n",
    "3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "5. 다시 양쪽 공백을 지웁니다\n",
    "6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d3a081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "#정제함수 만들기\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068ccd6",
   "metadata": {},
   "source": [
    "## 소스 문장(X_train)과 타겟 문장(y_train)으로 나누기\n",
    "소스문장은 입력값, 타겟문장은 출력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52325bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence) # 들어온 한 문장에 위의 함수처리한걸 받는 인자\n",
    "    corpus.append(preprocessed_sentence) #함수처리한 str값을 corpus 리스트에 집어넣기\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4f91a",
   "metadata": {},
   "source": [
    "## 데이터 벡터화(텐서화) - str 에서 숫자로\n",
    "데이터를 숫자로 변환하기. 이 과정을 벡터화(vectorize) 라 하며, 숫자로 변환된 데이터를 텐서(tensor) 라고 칭한다. 우리가 사용하는 텐서플로우로 만든 모델의 입출력 데이터는 실제로는 모두 이런 텐서로 변환되어 처리되는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9dd1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7f52903acdc0> \n",
      " [[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus): #corpus : 정제된 문장str 들의 리스트\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, #인덱스 최대치\n",
    "        filters=' ', #필터 필요 없어서 비워둠\n",
    "        oov_token=\"<unk>\" # 최대치\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  #post는 뒤에다가 패딩 붙이기\n",
    "    \n",
    "    print(tokenizer,\"\\n\" ,tensor)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1188fb2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "#토크나이저 내부 인덱스별 할당된 단어 모음 1~10까지만 보기\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b8dd7",
   "metadata": {},
   "source": [
    "#### corpus tensor 모든 로우의 시작값이 2, 문장 종료값이 3, 나머지는 0인 이유?\n",
    "\n",
    "인덱스 2,3이 start와 end라서 모든 코퍼스 내 원소의 시작은 2, 끝지점은 3, 7000단어까지 인덱싱을 하고도 남은 단어는 1(unk), 남은 길이는 패딩인 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc33fc1",
   "metadata": {},
   "source": [
    "## 소스 문장 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97b22eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07a88a",
   "metadata": {},
   "source": [
    "corpus 내의 첫 번째 문장에 대해 생성된 소스와 타겟 문장을 확인해 보았다. 예상대로 소스는 2(<start>)에서 시작해서 3(<end>)으로 끝난 후 0(<pad>)로 채워져 있다. 하지만 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한 칸 시프트 한 형태를 가지고 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8714a",
   "metadata": {},
   "source": [
    "## tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성\n",
    "\n",
    "tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad4edbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)) #각 원소별로 자르기\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)                    # 해당 길이에서 랜덤으로 원소 나열\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # 해당 값만큼의 연속된 원소들 묶음\n",
    "\n",
    "#만약 데이터가 같은 크기(차원)을 가졌다면, drop-remainder를 True해야함 - 나머지를 버리는 거\n",
    "#배치 사이즈로 균등히 나누다 보면, 마지막에 배치 사이즈보다 작아서 안나눠지는 원소들이 있다. 이걸 안없애면\n",
    "#배치 크기가 안맞아서 오류가 날수도. 패딩했으면 무조건 true로 해줘야할듯하다.\n",
    "dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60d268",
   "metadata": {},
   "source": [
    "20개의 단어로 이루어져있는 문장들 256개씩 그룹화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4201bbf9",
   "metadata": {},
   "source": [
    "## LSTM 모델설명\n",
    "\n",
    "우리 입력 텐서에는 단어 사전의 인덱스가 들어 있다. Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔 준다. 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6eec7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TextGenerator at 0x7f51ff79aeb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__() #얘는 무슨 초기화값을 상속받는것인가\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size) #단어책 사이즈로 줄여줘야 다음에 무슨 단어를 낼지에 대한 각 클래스별 수치 나옴\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 #워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 1024 #hidden state의 차원 수 : 판단 횟수? or 판단 다양성? 판단하는 두뇌 수?\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) #모델 정의\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1740aa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[ 1.34002359e-04, -2.53947519e-06,  1.90805848e-04, ...,\n",
       "         -1.02456986e-04, -2.89600459e-04,  6.83239632e-05],\n",
       "        [-1.73283508e-04, -4.12105255e-05,  3.18135833e-04, ...,\n",
       "         -5.04858326e-05, -3.94076982e-04,  1.03947845e-04],\n",
       "        [ 1.47684128e-04,  7.28819723e-05,  6.51891169e-04, ...,\n",
       "          1.58902796e-04, -5.07580990e-04,  2.56318162e-04],\n",
       "        ...,\n",
       "        [ 1.81447528e-03,  2.58446997e-03,  1.61241984e-03, ...,\n",
       "         -3.71697824e-03, -6.65375264e-04, -2.15717984e-04],\n",
       "        [ 1.97368045e-03,  2.81282584e-03,  1.73553848e-03, ...,\n",
       "         -4.37446218e-03, -8.47862801e-04, -6.10842661e-04],\n",
       "        [ 2.12585763e-03,  3.01645370e-03,  1.82154018e-03, ...,\n",
       "         -4.95731132e-03, -1.01631426e-03, -9.72796581e-04]],\n",
       "\n",
       "       [[ 1.34002359e-04, -2.53947519e-06,  1.90805848e-04, ...,\n",
       "         -1.02456986e-04, -2.89600459e-04,  6.83239632e-05],\n",
       "        [ 1.52306268e-06,  4.87122124e-05,  3.81461025e-04, ...,\n",
       "         -4.36633272e-05, -2.63580820e-04,  3.62203893e-04],\n",
       "        [ 2.62615009e-04, -4.46999329e-05,  6.43351639e-04, ...,\n",
       "          3.61871898e-05, -5.80168504e-04,  7.51414977e-04],\n",
       "        ...,\n",
       "        [ 1.56187837e-03,  2.56908033e-03,  1.68062595e-03, ...,\n",
       "         -4.91729379e-03, -1.21026987e-03, -7.45122845e-04],\n",
       "        [ 1.68644858e-03,  2.83616735e-03,  1.77602225e-03, ...,\n",
       "         -5.39734680e-03, -1.34785683e-03, -1.11553236e-03],\n",
       "        [ 1.82005111e-03,  3.06729344e-03,  1.85124017e-03, ...,\n",
       "         -5.81811974e-03, -1.47027231e-03, -1.43949711e-03]],\n",
       "\n",
       "       [[ 1.34002359e-04, -2.53947519e-06,  1.90805848e-04, ...,\n",
       "         -1.02456986e-04, -2.89600459e-04,  6.83239632e-05],\n",
       "        [ 3.01535154e-04,  4.09408298e-04,  3.54628079e-04, ...,\n",
       "         -4.51310771e-05, -3.27775808e-04,  3.92553367e-04],\n",
       "        [ 2.33842395e-04,  9.40907572e-04,  3.47646535e-04, ...,\n",
       "         -1.09971123e-04, -4.58940951e-04,  5.79445914e-04],\n",
       "        ...,\n",
       "        [ 9.76222625e-04,  1.57294248e-03,  2.00963370e-03, ...,\n",
       "         -4.45115333e-03, -5.90407872e-04, -5.21900191e-04],\n",
       "        [ 1.22871716e-03,  1.97007041e-03,  2.10106419e-03, ...,\n",
       "         -4.95629245e-03, -7.24467332e-04, -9.02492844e-04],\n",
       "        [ 1.46427262e-03,  2.31994642e-03,  2.16011750e-03, ...,\n",
       "         -5.40358806e-03, -8.55098770e-04, -1.24770228e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.34002359e-04, -2.53947519e-06,  1.90805848e-04, ...,\n",
       "         -1.02456986e-04, -2.89600459e-04,  6.83239632e-05],\n",
       "        [ 6.53709576e-05, -1.79462120e-04,  9.51862385e-05, ...,\n",
       "         -2.03761112e-04, -8.28738790e-04,  5.47103118e-05],\n",
       "        [ 2.00990427e-04, -9.50301182e-05,  1.86117075e-04, ...,\n",
       "         -3.66230146e-04, -1.12814130e-03,  3.43800377e-04],\n",
       "        ...,\n",
       "        [ 8.17060762e-04, -2.17615787e-04,  1.55590137e-03, ...,\n",
       "         -1.64856133e-03,  5.26398711e-04, -1.13135633e-04],\n",
       "        [ 9.74738854e-04,  7.58887618e-05,  1.71936606e-03, ...,\n",
       "         -2.29030242e-03,  2.09219972e-04, -3.05572234e-04],\n",
       "        [ 1.12521322e-03,  4.38688003e-04,  1.86426018e-03, ...,\n",
       "         -2.94735096e-03, -9.23614498e-05, -5.48334501e-04]],\n",
       "\n",
       "       [[ 1.34002359e-04, -2.53947519e-06,  1.90805848e-04, ...,\n",
       "         -1.02456986e-04, -2.89600459e-04,  6.83239632e-05],\n",
       "        [-1.97613437e-04, -3.67133725e-05,  4.46127902e-04, ...,\n",
       "         -5.02276598e-05, -1.55123373e-04,  5.42375392e-06],\n",
       "        [-3.66452150e-04,  1.34900256e-04,  5.26452495e-04, ...,\n",
       "         -1.17387601e-04, -5.23373950e-04,  6.34428725e-05],\n",
       "        ...,\n",
       "        [ 8.01624614e-04,  8.05669290e-04,  1.11735868e-03, ...,\n",
       "         -3.29086231e-03,  4.04818973e-04, -5.06298500e-04],\n",
       "        [ 9.21359053e-04,  1.21258630e-03,  1.41932082e-03, ...,\n",
       "         -3.89149552e-03,  1.01201338e-04, -8.94692668e-04],\n",
       "        [ 1.07694126e-03,  1.61203032e-03,  1.65812543e-03, ...,\n",
       "         -4.46026260e-03, -1.84384931e-04, -1.26091321e-03]],\n",
       "\n",
       "       [[ 1.34002359e-04, -2.53947519e-06,  1.90805848e-04, ...,\n",
       "         -1.02456986e-04, -2.89600459e-04,  6.83239632e-05],\n",
       "        [ 6.63091123e-05, -3.56381992e-04,  2.57471111e-04, ...,\n",
       "         -1.58708222e-04, -5.38306718e-04, -9.82221973e-05],\n",
       "        [-8.62328670e-05, -6.53365103e-04,  5.83337896e-05, ...,\n",
       "         -2.63756781e-04, -1.03599264e-03, -2.37866538e-04],\n",
       "        ...,\n",
       "        [ 9.20189545e-04,  2.09379083e-04,  1.14336994e-03, ...,\n",
       "         -2.40505300e-03, -5.07171208e-04,  1.67040780e-04],\n",
       "        [ 1.06785889e-03,  6.69254980e-04,  1.38818379e-03, ...,\n",
       "         -3.13522201e-03, -5.92603697e-04, -2.10845756e-04],\n",
       "        [ 1.23863446e-03,  1.11654494e-03,  1.57740200e-03, ...,\n",
       "         -3.80838057e-03, -6.80744124e-04, -5.96693542e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4189e0",
   "metadata": {},
   "source": [
    "shape=(256, 20, 7001)\n",
    "7001 - dense layer 출력 차원수 : 다음에 무슨 단어가 올 확률이 높은가?\n",
    "\n",
    "256 - 배치사이즈, 256개의 문장 가져옴 take(1) 이므로 배치 1번\n",
    "\n",
    "20 - rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)에서 return_sequences=True와 연관있다. 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스 출력하라는 의미 return_sequences=False 면 1로 출력됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
